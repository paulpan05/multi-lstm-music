{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Generation with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pickle\n",
    "import mido\n",
    "import random\n",
    "from mido import MidiFile, Message, MidiTrack\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Activation, Bidirectional, LayerNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "from keras_self_attention import SeqSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LUT for array indices storing note bits\n",
    "channels = [0,1,2,9]\n",
    "chanToArr = [0,1,2,0,0,0,0,0,0,3]\n",
    "\n",
    "def parse( input_midi ):\n",
    "    i = 1               # counter for each print statement\n",
    "    listo = []\n",
    "\n",
    "    prevChan = 0        # keeps track of previous channel to detect channel changes\n",
    "    chanIt = 0          # iterates to the next channel in the forced order 0->1->2->9\n",
    "    arr = [[0]*128]*4    # 12 notes each for channels(0,1,2,9) to keep track of continued notes\n",
    "\n",
    "    # insert MIDI file here\n",
    "    mid = MidiFile(input_midi) \n",
    "    temp = mido.merge_tracks(mid.tracks)\n",
    "\n",
    "    # processing MIDI file\n",
    "    for msg in temp:\n",
    "        # look for note changes\n",
    "        if(msg.type=='note_on'):\n",
    "\n",
    "            # check for a new channel\n",
    "            if(msg.channel != prevChan): \n",
    "                chanIt = chanIt + 1\n",
    "                if(chanIt == 4):\n",
    "                    chanIt = chanIt -4\n",
    "                    listo.append([0,0,0])\n",
    "                    i=i+1\n",
    "\n",
    "                # iterate chanIt to \"catch up\" to the next listed channel\n",
    "                while( channels[chanIt] != msg.channel):\n",
    "\n",
    "                    # continue notes if not turned off\n",
    "                    for a in range(len(arr[chanToArr[ channels[chanIt] ]])):\n",
    "                        if(arr[chanToArr[ channels[chanIt] ]][a]>0):\n",
    "                            listo[ arr[chanToArr[ channels[chanIt] ]][a] - 1 ][2] = listo[ arr[chanToArr[ channels[chanIt] ]][a] - 1 ][2] + 1\n",
    "                    chanIt = chanIt + 1\n",
    "                    if(chanIt == 4):\n",
    "                        chanIt = chanIt -4\n",
    "                        listo.append([0,0,0])\n",
    "                        i=i+1\n",
    "                    \n",
    "            # check if note_on event is on or off switch\n",
    "            if(msg.velocity==0):\n",
    "                noteSwitch = \"off\"\n",
    "                arr[chanToArr[msg.channel]][msg.note] = 0\n",
    "            else:\n",
    "                noteSwitch = \"on\"\n",
    "                arr[chanToArr[msg.channel]][msg.note] = i        \n",
    "                # print new note\n",
    "                listo.append([msg.note , msg.channel, 1])\n",
    "                i=i+1\n",
    "\n",
    "            # update prevChan for detection\n",
    "            prevChan = msg.channel\n",
    "\n",
    "    for t in range(len(listo)):\n",
    "        listo[t] = str(listo[t][0]) + \".\" + str(listo[t][1]) + \".\" + str(listo[t][2])\n",
    "        \n",
    "    return listo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notes():\n",
    "    i=0\n",
    "    notes = None\n",
    "    for file in glob.glob(\"samples/large/*.mid\"):\n",
    "\n",
    "        print(\"Parsing %s\" % file)        \n",
    "        if(i==0):\n",
    "            notes = parse(file)\n",
    "            i=i+1\n",
    "        else:\n",
    "            notes = np.concatenate((notes, parse(file)))\n",
    "    \n",
    "    pickle.dump(notes, open('notes.p', 'wb'))\n",
    "    return notes\n",
    "\n",
    "def prepare_sequences(notes, n_vocab):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "    sequence_length = 16\n",
    "\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "    \n",
    "    network_input = []\n",
    "    network_output = []\n",
    "\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "    \n",
    "    n_patterns = len(network_input)\n",
    "    # reshape the input into a format compatible with LSTM layers\n",
    "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "\n",
    "    # normalize input\n",
    "    network_input = network_input / float(n_vocab)\n",
    "\n",
    "    network_output = to_categorical(network_output)\n",
    "    \n",
    "    return (network_input, network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(network_input, n_vocab):\n",
    "    \"\"\" create the structure of the neural network \"\"\"\n",
    "    hidden_layers = 256\n",
    "    dropout = 0.4\n",
    "    \n",
    "    \"\"\" Initializing model \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    \"\"\" Adding LSTM Layers to Model \"\"\"\n",
    "    model.add(\n",
    "        Bidirectional(\n",
    "            LSTM(\n",
    "                hidden_layers,\n",
    "                dropout=dropout,\n",
    "                return_sequences=True\n",
    "            ),\n",
    "            input_shape=(network_input.shape[1], network_input.shape[2])\n",
    "        )\n",
    "    )\n",
    "    model.add(SeqSelfAttention())\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(\n",
    "        Bidirectional(\n",
    "            LSTM(\n",
    "                hidden_layers,\n",
    "                dropout=dropout,\n",
    "                return_sequences=True\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    model.add(SeqSelfAttention())\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(\n",
    "        Bidirectional(\n",
    "            LSTM(\n",
    "                hidden_layers,\n",
    "                dropout=dropout\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    model.add(LayerNormalization())\n",
    "    \n",
    "    \"\"\" Add other layers after LSTM \"\"\"\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(hidden_layers))\n",
    "    # model.add(Activation('relu'))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(n_vocab))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(clipnorm=1.0))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(load_notes=False, to_load_model=False, learning_rate=None):\n",
    "    \"\"\" Train a Neural Network to generate music \"\"\"\n",
    "    notes = None\n",
    "    model = None\n",
    "\n",
    "    if load_notes:\n",
    "        notes = pickle.load(open('notes.p', 'rb'))\n",
    "    else:\n",
    "        notes = get_notes()\n",
    "    \n",
    "    n_vocab = len(set(notes))\n",
    "    network_input, network_output = prepare_sequences(notes, n_vocab)\n",
    "\n",
    "    if to_load_model:\n",
    "        model = load_model('weights.hdf5', custom_objects={'SeqSelfAttention': SeqSelfAttention})\n",
    "    else:\n",
    "        model = create_network(network_input, n_vocab)\n",
    "\n",
    "    if learning_rate != None:\n",
    "        K.set_value(model.optimizer.learning_rate, learning_rate)\n",
    "\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        'weights.hdf5',\n",
    "        monitor='loss',\n",
    "        save_best_only=True,\n",
    "        mode='min'\n",
    "    )\n",
    "\n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    model.fit(\n",
    "        x=network_input,\n",
    "        y=network_output,\n",
    "        batch_size=32,\n",
    "        epochs=3000,\n",
    "        callbacks=callbacks_list\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing samples/large/091_DragonWarriorIV_40_41HornofBaron.mid\n",
      "Parsing samples/large/090_DragonWarriorIII_06_07Victory.mid\n",
      "Parsing samples/large/090_DragonWarriorIII_05_06BattleTheme.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_09_10MysteriousShrine.mid\n",
      "Parsing samples/large/090_DragonWarriorIII_19_20Cursed.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_19_20Jackpot.mid\n",
      "Parsing samples/large/090_DragonWarriorIII_30_31IntoTheLegend.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_03_04SolitaryWarrior.mid\n",
      "Parsing samples/large/088_DragonWarrior_00_01Overture.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_27_28ExpandingtheSeaMap.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_39_40TheUnknownCastle.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_06_07ImportantItemDiscovery.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_05_06CaveofFear.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_37_38BalloonsFlight.mid\n",
      "Parsing samples/large/090_DragonWarriorIII_03_04Rondo.mid\n",
      "Parsing samples/large/090_DragonWarriorIII_17_18Jipang.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_26_27Departure.mid\n",
      "Parsing samples/large/088_DragonWarrior_17_18CaveB4F.mid\n",
      "Parsing samples/large/089_DragonWarriorII_04_05Deathfight.mid\n",
      "Parsing samples/large/090_DragonWarriorIII_14_15Requiem.mid\n",
      "Parsing samples/large/090_DragonWarriorIII_15_16Church.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_29_30Elegy.mid\n",
      "Parsing samples/large/088_DragonWarrior_01_02People.mid\n",
      "Parsing samples/large/089_DragonWarriorII_17_18UnknownWorld.mid\n",
      "Parsing samples/large/090_DragonWarriorIII_22_23RainbowBridge.mid\n",
      "Parsing samples/large/089_DragonWarriorII_19_20EchoingFlute1.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_32_33CursedTower.mid\n",
      "Parsing samples/large/088_DragonWarrior_16_17CaveB3F.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_24_25Inn.mid\n",
      "Parsing samples/large/089_DragonWarriorII_02_03Chateau.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_38_39FluteofUncovering.mid\n",
      "Parsing samples/large/089_DragonWarriorII_05_06Victory.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_35_36PadequiaSeed.mid\n",
      "Parsing samples/large/089_DragonWarriorII_16_17BeyondtheWaves.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_18_19APleasantCasino.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_22_23Chapter3Ending.mid\n",
      "Parsing samples/large/088_DragonWarrior_07_08LevelUp.mid\n",
      "Parsing samples/large/090_DragonWarriorIII_04_05Adventure.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_15_16Chapter2Ending.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_46_47Curse.mid\n",
      "Parsing samples/large/090_DragonWarriorIII_13_14Pyramid.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_41_42ViciousThing.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_01_02Intermezzo.mid\n",
      "Parsing samples/large/090_DragonWarriorIII_31_32EchoingFlute1.mid\n",
      "Parsing samples/large/088_DragonWarrior_18_19CaveB5F.mid\n",
      "Parsing samples/large/088_DragonWarrior_08_09Inn.mid\n",
      "Parsing samples/large/090_DragonWarriorIII_24_25RainbowStaff.mid\n",
      "Parsing samples/large/089_DragonWarriorII_03_04DistantJourney.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_12_13BattleFortheGlory.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_44_45ColosseumStandWithoutIntro.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_00_01Overture.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_10_11ColosseumDressingRoom.mid\n",
      "Parsing samples/large/089_DragonWarriorII_01_02OnlyLonelyBoy.mid\n",
      "Parsing samples/large/089_DragonWarriorII_10_11Inn.mid\n",
      "Parsing samples/large/090_DragonWarriorIII_25_26DeathofOrtega.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_45_46Church.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_04_05Town.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_16_17ArmsMerchantToruneko.mid\n",
      "Parsing samples/large/089_DragonWarriorII_09_10Encounter.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_25_26GypsyTrip.mid\n",
      "Parsing samples/large/089_DragonWarriorII_11_12HolyShrine.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_21_22SuperBigJackpot.mid\n",
      "Parsing samples/large/089_DragonWarriorII_24_25MyRoadMyJourney.mid\n",
      "Parsing samples/large/089_DragonWarriorII_20_21EchoingFlute2.mid\n",
      "Parsing samples/large/088_DragonWarrior_23_24Finale.mid\n",
      "Parsing samples/large/090_DragonWarriorIII_20_21CurseCured.mid\n",
      "Parsing samples/large/090_DragonWarriorIII_10_11Village.mid\n",
      "Parsing samples/large/089_DragonWarriorII_00_01Overture.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_08_09PrincessAlenasAdventure.mid\n",
      "Parsing samples/large/088_DragonWarrior_20_21CaveB7F.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_30_31Homeland.mid\n",
      "Parsing samples/large/090_DragonWarriorIII_02_03Town.mid\n",
      "Parsing samples/large/089_DragonWarriorII_22_23Curse.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_31_32Encounter.mid\n",
      "Parsing samples/large/090_DragonWarriorIII_33_34Curse.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_28_29Chapter4Ending.mid\n",
      "Parsing samples/large/090_DragonWarriorIII_29_30Loto.mid\n",
      "Parsing samples/large/090_DragonWarriorIII_09_10Tower.mid\n",
      "Parsing samples/large/088_DragonWarrior_19_20CaveB6F.mid\n",
      "Parsing samples/large/090_DragonWarriorIII_18_19ThePhantomShip.mid\n",
      "Parsing samples/large/090_DragonWarriorIII_16_17Sailing.mid\n",
      "Parsing samples/large/090_DragonWarriorIII_27_28MorninginAlefguard.mid\n",
      "Parsing samples/large/090_DragonWarriorIII_26_27FightingSpirit.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_42_43IncarnationofEvil.mid\n",
      "Parsing samples/large/088_DragonWarrior_21_22CaveB8F.mid\n",
      "Parsing samples/large/088_DragonWarrior_14_15CaveB1F.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_17_18Save.mid\n",
      "Parsing samples/large/090_DragonWarriorIII_12_13SmallShrine.mid\n",
      "Parsing samples/large/088_DragonWarrior_11_12GwaelinsLove.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_13_14Victory.mid\n",
      "Parsing samples/large/088_DragonWarrior_09_10Death.mid\n",
      "Parsing samples/large/090_DragonWarriorIII_01_02Intermezzo.mid\n",
      "Parsing samples/large/088_DragonWarrior_15_16CaveB2F.mid\n",
      "Parsing samples/large/089_DragonWarriorII_12_13Requiem.mid\n",
      "Parsing samples/large/090_DragonWarriorIII_21_22HeavenlyFlight.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_43_44FinaleGuidingPeople.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_20_21BigJackpot.mid\n",
      "Parsing samples/large/089_DragonWarriorII_14_15EndlessWorld.mid\n",
      "Parsing samples/large/088_DragonWarrior_04_05UnknownWorld.mid\n",
      "Parsing samples/large/089_DragonWarriorII_18_19LotterySuccess.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_14_15LevelUp.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_02_03Minuet.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_11_12ColosseumStand.mid\n",
      "Parsing samples/large/088_DragonWarrior_05_06Battle.mid\n",
      "Parsing samples/large/090_DragonWarriorIII_08_09Dungeon.mid\n",
      "Parsing samples/large/088_DragonWarrior_24_25Curse.mid\n",
      "Parsing samples/large/090_DragonWarriorIII_28_29UnknownWorld.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_23_24GypsyDance.mid\n",
      "Parsing samples/large/089_DragonWarriorII_08_09FrightinDungeon.mid\n",
      "Parsing samples/large/090_DragonWarriorIII_32_33EchoingFlute2.mid\n",
      "Parsing samples/large/089_DragonWarriorII_15_16DevilsTower.mid\n",
      "Parsing samples/large/089_DragonWarriorII_07_08Town.mid\n",
      "Parsing samples/large/089_DragonWarriorII_21_22CharmofRubiss.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_07_08Chapter1Ending.mid\n",
      "Parsing samples/large/090_DragonWarriorIII_11_12Inn.mid\n",
      "Parsing samples/large/088_DragonWarrior_12_13RainbowBridge.mid\n",
      "Parsing samples/large/090_DragonWarriorIII_07_08LevelUp.mid\n",
      "Parsing samples/large/088_DragonWarrior_03_04ChateauLadutormHall.mid\n",
      "Parsing samples/large/089_DragonWarriorII_23_24DeadorAlive.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_33_34SagesStone.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_36_37DiseaseRestoration.mid\n",
      "Parsing samples/large/088_DragonWarrior_02_03ChateauLadutorm.mid\n",
      "Parsing samples/large/088_DragonWarrior_22_23KingDragon.mid\n",
      "Parsing samples/large/091_DragonWarriorIV_34_35WagonWheelsMarch.mid\n",
      "Parsing samples/large/088_DragonWarrior_06_07Victory.mid\n",
      "Parsing samples/large/090_DragonWarriorIII_00_01Title.mid\n",
      "Parsing samples/large/089_DragonWarriorII_06_07LevelUp.mid\n",
      "Parsing samples/large/089_DragonWarriorII_13_14Church.mid\n",
      "Parsing samples/large/088_DragonWarrior_13_14RainbowStaff.mid\n",
      "Parsing samples/large/090_DragonWarriorIII_23_24SleepFlute.mid\n",
      "Parsing samples/large/088_DragonWarrior_10_11SleepFlute.mid\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional (Bidirectional (None, 16, 512)           528384    \n",
      "_________________________________________________________________\n",
      "seq_self_attention (SeqSelfA (None, None, 512)         32833     \n",
      "_________________________________________________________________\n",
      "layer_normalization (LayerNo (None, None, 512)         1024      \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 512)         1574912   \n",
      "_________________________________________________________________\n",
      "seq_self_attention_1 (SeqSel (None, None, 512)         32833     \n",
      "_________________________________________________________________\n",
      "layer_normalization_1 (Layer (None, None, 512)         1024      \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 512)               1574912   \n",
      "_________________________________________________________________\n",
      "layer_normalization_2 (Layer (None, 512)               1024      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "layer_normalization_3 (Layer (None, 256)               512       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1295)              332815    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 1295)              0         \n",
      "=================================================================\n",
      "Total params: 4,211,601\n",
      "Trainable params: 4,211,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1920/1920 [==============================] - 50s 26ms/step - loss: 5.0977\n",
      "Epoch 2/3000\n",
      "1920/1920 [==============================] - 50s 26ms/step - loss: 4.8426\n",
      "Epoch 3/3000\n",
      "1920/1920 [==============================] - 49s 26ms/step - loss: 4.7863\n",
      "Epoch 4/3000\n",
      "1920/1920 [==============================] - 50s 26ms/step - loss: 4.7539\n",
      "Epoch 5/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 4.7003\n",
      "Epoch 6/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 4.6370\n",
      "Epoch 7/3000\n",
      "1920/1920 [==============================] - 51s 27ms/step - loss: 4.5967\n",
      "Epoch 8/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 4.5696\n",
      "Epoch 9/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 4.5410\n",
      "Epoch 10/3000\n",
      "1920/1920 [==============================] - 50s 26ms/step - loss: 4.5146\n",
      "Epoch 11/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 4.4889\n",
      "Epoch 12/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 4.4657\n",
      "Epoch 13/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 4.4404\n",
      "Epoch 14/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 4.4200\n",
      "Epoch 15/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 4.4013\n",
      "Epoch 16/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 4.3821\n",
      "Epoch 17/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 4.3667\n",
      "Epoch 18/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 4.3482\n",
      "Epoch 19/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 4.3280\n",
      "Epoch 20/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 4.3128\n",
      "Epoch 21/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 4.2979\n",
      "Epoch 22/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 4.2786\n",
      "Epoch 23/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 4.2575\n",
      "Epoch 24/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 4.2446\n",
      "Epoch 25/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 4.2329\n",
      "Epoch 26/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 4.2134\n",
      "Epoch 27/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 4.1963\n",
      "Epoch 28/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 4.1866\n",
      "Epoch 29/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 4.1737\n",
      "Epoch 30/3000\n",
      "1920/1920 [==============================] - 51s 27ms/step - loss: 4.1567\n",
      "Epoch 31/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 4.1444\n",
      "Epoch 32/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 4.1341\n",
      "Epoch 33/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 4.1251\n",
      "Epoch 34/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 4.1117\n",
      "Epoch 35/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 4.1049\n",
      "Epoch 36/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 4.0884\n",
      "Epoch 37/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 4.0794\n",
      "Epoch 38/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 4.0685\n",
      "Epoch 39/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 4.0610\n",
      "Epoch 40/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 4.0524\n",
      "Epoch 41/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 4.0443\n",
      "Epoch 42/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 4.0266\n",
      "Epoch 43/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 4.0197\n",
      "Epoch 44/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 4.0113\n",
      "Epoch 45/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.9960\n",
      "Epoch 46/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.9994\n",
      "Epoch 47/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.9798\n",
      "Epoch 48/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.9709\n",
      "Epoch 49/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.9687\n",
      "Epoch 50/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.9531\n",
      "Epoch 51/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.9476\n",
      "Epoch 52/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.9351\n",
      "Epoch 53/3000\n",
      "1920/1920 [==============================] - 51s 26ms/step - loss: 3.9309\n",
      "Epoch 54/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.9266\n",
      "Epoch 55/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.9179\n",
      "Epoch 56/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.9036\n",
      "Epoch 57/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.9032\n",
      "Epoch 58/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.8906\n",
      "Epoch 59/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.8848\n",
      "Epoch 60/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.8752\n",
      "Epoch 61/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.8691\n",
      "Epoch 62/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.8640\n",
      "Epoch 63/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.8616\n",
      "Epoch 64/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.8480\n",
      "Epoch 65/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.8382\n",
      "Epoch 66/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.8292\n",
      "Epoch 67/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.8268\n",
      "Epoch 68/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.8139\n",
      "Epoch 69/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.8124\n",
      "Epoch 70/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.8102\n",
      "Epoch 71/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.8032\n",
      "Epoch 72/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.7882\n",
      "Epoch 73/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.7850\n",
      "Epoch 74/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.7836\n",
      "Epoch 75/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.7736\n",
      "Epoch 76/3000\n",
      "1920/1920 [==============================] - 51s 27ms/step - loss: 3.7634\n",
      "Epoch 77/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.7534\n",
      "Epoch 78/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.7527\n",
      "Epoch 79/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.7414\n",
      "Epoch 80/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.7334\n",
      "Epoch 81/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.7412\n",
      "Epoch 82/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.7228\n",
      "Epoch 83/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.7207\n",
      "Epoch 84/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.7219\n",
      "Epoch 85/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.7079\n",
      "Epoch 86/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.7018\n",
      "Epoch 87/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.6961\n",
      "Epoch 88/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.6942\n",
      "Epoch 89/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.6849\n",
      "Epoch 90/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.6837\n",
      "Epoch 91/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.6778\n",
      "Epoch 92/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.6676\n",
      "Epoch 93/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.6650\n",
      "Epoch 94/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.6605\n",
      "Epoch 95/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.6487\n",
      "Epoch 96/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.6530\n",
      "Epoch 97/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.6428\n",
      "Epoch 98/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.6432\n",
      "Epoch 99/3000\n",
      "1920/1920 [==============================] - 51s 27ms/step - loss: 3.6290\n",
      "Epoch 100/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.6224\n",
      "Epoch 101/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.6180\n",
      "Epoch 102/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.6093\n",
      "Epoch 103/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.6167\n",
      "Epoch 104/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.5994\n",
      "Epoch 105/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.5949\n",
      "Epoch 106/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.5826\n",
      "Epoch 107/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.5814\n",
      "Epoch 108/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.5864\n",
      "Epoch 109/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.5784\n",
      "Epoch 110/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.5758\n",
      "Epoch 111/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.5675\n",
      "Epoch 112/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.5715\n",
      "Epoch 113/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.5607\n",
      "Epoch 114/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.5553\n",
      "Epoch 115/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.5490\n",
      "Epoch 116/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.5447\n",
      "Epoch 117/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.5456\n",
      "Epoch 118/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.5444\n",
      "Epoch 119/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.5367\n",
      "Epoch 120/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.5267\n",
      "Epoch 121/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.5309\n",
      "Epoch 122/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.5255\n",
      "Epoch 123/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.5135\n",
      "Epoch 124/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.5074\n",
      "Epoch 125/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.5064\n",
      "Epoch 126/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.5017\n",
      "Epoch 127/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.4989\n",
      "Epoch 128/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.4969\n",
      "Epoch 129/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.4927\n",
      "Epoch 130/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.4829\n",
      "Epoch 131/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.4871\n",
      "Epoch 132/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.4771\n",
      "Epoch 133/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.4683\n",
      "Epoch 134/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.4672\n",
      "Epoch 135/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.4683\n",
      "Epoch 136/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.4650\n",
      "Epoch 137/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.4639\n",
      "Epoch 138/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.4577\n",
      "Epoch 139/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.4481\n",
      "Epoch 140/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.4465\n",
      "Epoch 141/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.4321\n",
      "Epoch 142/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.4394\n",
      "Epoch 143/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.4330\n",
      "Epoch 144/3000\n",
      "1920/1920 [==============================] - 51s 27ms/step - loss: 3.4301\n",
      "Epoch 145/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.4321\n",
      "Epoch 146/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.4242\n",
      "Epoch 147/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.4135\n",
      "Epoch 148/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.4133\n",
      "Epoch 149/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.4128\n",
      "Epoch 150/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.4051\n",
      "Epoch 151/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.3967\n",
      "Epoch 152/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.4075\n",
      "Epoch 153/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.3907\n",
      "Epoch 154/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.3968\n",
      "Epoch 155/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.3863\n",
      "Epoch 156/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.3825\n",
      "Epoch 157/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.3817\n",
      "Epoch 158/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.3827\n",
      "Epoch 159/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.3820\n",
      "Epoch 160/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.3792\n",
      "Epoch 161/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.3632\n",
      "Epoch 162/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.3733\n",
      "Epoch 163/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.3476\n",
      "Epoch 164/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.3598\n",
      "Epoch 165/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.3625\n",
      "Epoch 166/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.3507\n",
      "Epoch 167/3000\n",
      "1920/1920 [==============================] - 51s 27ms/step - loss: 3.3471\n",
      "Epoch 168/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.3436\n",
      "Epoch 169/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.3367\n",
      "Epoch 170/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.3287\n",
      "Epoch 171/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.3357\n",
      "Epoch 172/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.3274\n",
      "Epoch 173/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.3249\n",
      "Epoch 174/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.3270\n",
      "Epoch 175/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.3213\n",
      "Epoch 176/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.3095\n",
      "Epoch 177/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.3189\n",
      "Epoch 178/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.3090\n",
      "Epoch 179/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.3258\n",
      "Epoch 180/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.2990\n",
      "Epoch 181/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.2917\n",
      "Epoch 182/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.3050\n",
      "Epoch 183/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.2956\n",
      "Epoch 184/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.2930\n",
      "Epoch 185/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.2882\n",
      "Epoch 186/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.2748\n",
      "Epoch 187/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.2823\n",
      "Epoch 188/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.2768\n",
      "Epoch 189/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.2845\n",
      "Epoch 190/3000\n",
      "1920/1920 [==============================] - 51s 27ms/step - loss: 3.2809\n",
      "Epoch 191/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.2812\n",
      "Epoch 192/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.2721\n",
      "Epoch 193/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.2653\n",
      "Epoch 194/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.2741\n",
      "Epoch 195/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.2693\n",
      "Epoch 196/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.2556\n",
      "Epoch 197/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.2572\n",
      "Epoch 198/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.2626\n",
      "Epoch 199/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.2537\n",
      "Epoch 200/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.2448\n",
      "Epoch 201/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.2412\n",
      "Epoch 202/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.2542\n",
      "Epoch 203/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.2432\n",
      "Epoch 204/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.2307\n",
      "Epoch 205/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.2371\n",
      "Epoch 206/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.2337\n",
      "Epoch 207/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.2331\n",
      "Epoch 208/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.2381\n",
      "Epoch 209/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.2310\n",
      "Epoch 210/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.2237\n",
      "Epoch 211/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.2277\n",
      "Epoch 212/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.2256\n",
      "Epoch 213/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.2252\n",
      "Epoch 214/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.2188\n",
      "Epoch 215/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.2190\n",
      "Epoch 216/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.2187\n",
      "Epoch 217/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.2045\n",
      "Epoch 218/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.2108\n",
      "Epoch 219/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.2032\n",
      "Epoch 220/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.2036\n",
      "Epoch 221/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.2132\n",
      "Epoch 222/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.2036\n",
      "Epoch 223/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.2017\n",
      "Epoch 224/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.1968\n",
      "Epoch 225/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.1881\n",
      "Epoch 226/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.2014\n",
      "Epoch 227/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.1904\n",
      "Epoch 228/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.1831\n",
      "Epoch 229/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.1835\n",
      "Epoch 230/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.1805\n",
      "Epoch 231/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.1834\n",
      "Epoch 232/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.1770\n",
      "Epoch 233/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.1808\n",
      "Epoch 234/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.1830\n",
      "Epoch 235/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.1681\n",
      "Epoch 236/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.1672\n",
      "Epoch 237/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.1863\n",
      "Epoch 238/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.1631\n",
      "Epoch 239/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.1636\n",
      "Epoch 240/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.1609\n",
      "Epoch 241/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.1594\n",
      "Epoch 242/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.1418\n",
      "Epoch 243/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.1548\n",
      "Epoch 244/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.1576\n",
      "Epoch 245/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.1616\n",
      "Epoch 246/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.1512\n",
      "Epoch 247/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.1572\n",
      "Epoch 248/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.1545\n",
      "Epoch 249/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.1424\n",
      "Epoch 250/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.1577\n",
      "Epoch 251/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.1519\n",
      "Epoch 252/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.1509\n",
      "Epoch 253/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.1465\n",
      "Epoch 254/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.1381\n",
      "Epoch 255/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.1426\n",
      "Epoch 256/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.1396\n",
      "Epoch 257/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.1288\n",
      "Epoch 258/3000\n",
      "1920/1920 [==============================] - 51s 27ms/step - loss: 3.1357\n",
      "Epoch 259/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.1251\n",
      "Epoch 260/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.1368\n",
      "Epoch 261/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.1296\n",
      "Epoch 262/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.1308\n",
      "Epoch 263/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.1216\n",
      "Epoch 264/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.1272\n",
      "Epoch 265/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.1276\n",
      "Epoch 266/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.1233\n",
      "Epoch 267/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.1223\n",
      "Epoch 268/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.1102\n",
      "Epoch 269/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.1052\n",
      "Epoch 270/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.1204\n",
      "Epoch 271/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.1192\n",
      "Epoch 272/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.1143\n",
      "Epoch 273/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.1126\n",
      "Epoch 274/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.1140\n",
      "Epoch 275/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.1148\n",
      "Epoch 276/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.1023\n",
      "Epoch 277/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.1142\n",
      "Epoch 278/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.1085\n",
      "Epoch 279/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0978\n",
      "Epoch 280/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.1030\n",
      "Epoch 281/3000\n",
      "1920/1920 [==============================] - 51s 27ms/step - loss: 3.1075\n",
      "Epoch 282/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0997\n",
      "Epoch 283/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.1014\n",
      "Epoch 284/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.1004\n",
      "Epoch 285/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0955\n",
      "Epoch 286/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0951\n",
      "Epoch 287/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.1017\n",
      "Epoch 288/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0984\n",
      "Epoch 289/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0911\n",
      "Epoch 290/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0944\n",
      "Epoch 291/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0937\n",
      "Epoch 292/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.1059\n",
      "Epoch 293/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0823\n",
      "Epoch 294/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0701\n",
      "Epoch 295/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0770\n",
      "Epoch 296/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0743\n",
      "Epoch 297/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0830\n",
      "Epoch 298/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.0889\n",
      "Epoch 299/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0849\n",
      "Epoch 300/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0707\n",
      "Epoch 301/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0781\n",
      "Epoch 302/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0740\n",
      "Epoch 303/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0748\n",
      "Epoch 304/3000\n",
      "1920/1920 [==============================] - 51s 27ms/step - loss: 3.0737\n",
      "Epoch 305/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0650\n",
      "Epoch 306/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0709\n",
      "Epoch 307/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0719\n",
      "Epoch 308/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0806\n",
      "Epoch 309/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0700\n",
      "Epoch 310/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0633\n",
      "Epoch 311/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0630\n",
      "Epoch 312/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0635\n",
      "Epoch 313/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0630\n",
      "Epoch 314/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0649\n",
      "Epoch 315/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.0638\n",
      "Epoch 316/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0632\n",
      "Epoch 317/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0786\n",
      "Epoch 318/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0679\n",
      "Epoch 319/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0582\n",
      "Epoch 320/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0642\n",
      "Epoch 321/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0519\n",
      "Epoch 322/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0562\n",
      "Epoch 323/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0529\n",
      "Epoch 324/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0517\n",
      "Epoch 325/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0464\n",
      "Epoch 326/3000\n",
      "1920/1920 [==============================] - 51s 27ms/step - loss: 3.0489\n",
      "Epoch 327/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0648\n",
      "Epoch 328/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0567\n",
      "Epoch 329/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0491\n",
      "Epoch 330/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0564\n",
      "Epoch 331/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0605\n",
      "Epoch 332/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.0608\n",
      "Epoch 333/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0489\n",
      "Epoch 334/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0489\n",
      "Epoch 335/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0538\n",
      "Epoch 336/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0484\n",
      "Epoch 337/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0466\n",
      "Epoch 338/3000\n",
      "1920/1920 [==============================] - 49s 26ms/step - loss: 3.0473\n",
      "Epoch 339/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0516\n",
      "Epoch 340/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0518\n",
      "Epoch 341/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0493\n",
      "Epoch 342/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0478\n",
      "Epoch 343/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0428\n",
      "Epoch 344/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0405\n",
      "Epoch 345/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0347\n",
      "Epoch 346/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0390\n",
      "Epoch 347/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.0419\n",
      "Epoch 348/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0371\n",
      "Epoch 349/3000\n",
      "1920/1920 [==============================] - 51s 27ms/step - loss: 3.0452\n",
      "Epoch 350/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0417\n",
      "Epoch 351/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0366\n",
      "Epoch 352/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0321\n",
      "Epoch 353/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0361\n",
      "Epoch 354/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0510\n",
      "Epoch 355/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0338\n",
      "Epoch 356/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0362\n",
      "Epoch 357/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0397\n",
      "Epoch 358/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0497\n",
      "Epoch 359/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0395\n",
      "Epoch 360/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0299\n",
      "Epoch 361/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.0454\n",
      "Epoch 362/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.0328\n",
      "Epoch 363/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0262\n",
      "Epoch 364/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0378\n",
      "Epoch 365/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0265\n",
      "Epoch 366/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.0297\n",
      "Epoch 367/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0280\n",
      "Epoch 368/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0292\n",
      "Epoch 369/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0323\n",
      "Epoch 370/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0314\n",
      "Epoch 371/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0314\n",
      "Epoch 372/3000\n",
      "1920/1920 [==============================] - 51s 27ms/step - loss: 3.0287\n",
      "Epoch 373/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0219\n",
      "Epoch 374/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0291\n",
      "Epoch 375/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0404\n",
      "Epoch 376/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0242\n",
      "Epoch 377/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0279\n",
      "Epoch 378/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0314\n",
      "Epoch 379/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0225\n",
      "Epoch 380/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0353\n",
      "Epoch 381/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0369\n",
      "Epoch 382/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0376\n",
      "Epoch 383/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.0287\n",
      "Epoch 384/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0315\n",
      "Epoch 385/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0274\n",
      "Epoch 386/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0326\n",
      "Epoch 387/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0281\n",
      "Epoch 388/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0345\n",
      "Epoch 389/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0164\n",
      "Epoch 390/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0361\n",
      "Epoch 391/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0262\n",
      "Epoch 392/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.0252\n",
      "Epoch 393/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0332\n",
      "Epoch 394/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0396\n",
      "Epoch 395/3000\n",
      "1920/1920 [==============================] - 51s 27ms/step - loss: 3.0407\n",
      "Epoch 396/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0332\n",
      "Epoch 397/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0278\n",
      "Epoch 398/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0254\n",
      "Epoch 399/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0254\n",
      "Epoch 400/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0297\n",
      "Epoch 401/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0248\n",
      "Epoch 402/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0238\n",
      "Epoch 403/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0306\n",
      "Epoch 404/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0265\n",
      "Epoch 405/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0230\n",
      "Epoch 406/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.0282\n",
      "Epoch 407/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0248\n",
      "Epoch 408/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0299\n",
      "Epoch 409/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0260\n",
      "Epoch 410/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0230\n",
      "Epoch 411/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0163\n",
      "Epoch 412/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.0170\n",
      "Epoch 413/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0200\n",
      "Epoch 414/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0260\n",
      "Epoch 415/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0258\n",
      "Epoch 416/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0178\n",
      "Epoch 417/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.0322\n",
      "Epoch 418/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.0268\n",
      "Epoch 419/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0347\n",
      "Epoch 420/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0259\n",
      "Epoch 421/3000\n",
      "1920/1920 [==============================] - 49s 26ms/step - loss: 3.0200\n",
      "Epoch 422/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0213\n",
      "Epoch 423/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.0176\n",
      "Epoch 424/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0314\n",
      "Epoch 425/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0283\n",
      "Epoch 426/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0354\n",
      "Epoch 427/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0204\n",
      "Epoch 428/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0146\n",
      "Epoch 429/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.0210\n",
      "Epoch 430/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0194\n",
      "Epoch 431/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0267\n",
      "Epoch 432/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0190\n",
      "Epoch 433/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0233\n",
      "Epoch 434/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0315\n",
      "Epoch 435/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0302\n",
      "Epoch 436/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0320\n",
      "Epoch 437/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0296\n",
      "Epoch 438/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0234\n",
      "Epoch 439/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0324\n",
      "Epoch 440/3000\n",
      "1920/1920 [==============================] - 51s 27ms/step - loss: 3.0387\n",
      "Epoch 441/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0295\n",
      "Epoch 442/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0237\n",
      "Epoch 443/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0244\n",
      "Epoch 444/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0397\n",
      "Epoch 445/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0287\n",
      "Epoch 446/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0235\n",
      "Epoch 447/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0148\n",
      "Epoch 448/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0250\n",
      "Epoch 449/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0193\n",
      "Epoch 450/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0279\n",
      "Epoch 451/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0200\n",
      "Epoch 452/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.0272\n",
      "Epoch 453/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0290\n",
      "Epoch 454/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0254\n",
      "Epoch 455/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0252\n",
      "Epoch 456/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0261\n",
      "Epoch 457/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0311\n",
      "Epoch 458/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0302\n",
      "Epoch 459/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0248\n",
      "Epoch 460/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0289\n",
      "Epoch 461/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0247\n",
      "Epoch 462/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0250\n",
      "Epoch 463/3000\n",
      "1920/1920 [==============================] - 51s 27ms/step - loss: 3.0202\n",
      "Epoch 464/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0345\n",
      "Epoch 465/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0210\n",
      "Epoch 466/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0277\n",
      "Epoch 467/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0217\n",
      "Epoch 468/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0163\n",
      "Epoch 469/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0292\n",
      "Epoch 470/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0353\n",
      "Epoch 471/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0225\n",
      "Epoch 472/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0371\n",
      "Epoch 473/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0384\n",
      "Epoch 474/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.0299\n",
      "Epoch 475/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.0371\n",
      "Epoch 476/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0343\n",
      "Epoch 477/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0368\n",
      "Epoch 478/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0418\n",
      "Epoch 479/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0263\n",
      "Epoch 480/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.0322\n",
      "Epoch 481/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0228\n",
      "Epoch 482/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0332\n",
      "Epoch 483/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0500\n",
      "Epoch 484/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0338\n",
      "Epoch 485/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0471\n",
      "Epoch 486/3000\n",
      "1920/1920 [==============================] - 51s 27ms/step - loss: 3.0401\n",
      "Epoch 487/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0335\n",
      "Epoch 488/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0494\n",
      "Epoch 489/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0437\n",
      "Epoch 490/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0401\n",
      "Epoch 491/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0421\n",
      "Epoch 492/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0298\n",
      "Epoch 493/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0498\n",
      "Epoch 494/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0420\n",
      "Epoch 495/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0280\n",
      "Epoch 496/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0395\n",
      "Epoch 497/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.0346\n",
      "Epoch 498/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0426\n",
      "Epoch 499/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0393\n",
      "Epoch 500/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0436\n",
      "Epoch 501/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0389\n",
      "Epoch 502/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0438\n",
      "Epoch 503/3000\n",
      "1920/1920 [==============================] - 51s 27ms/step - loss: 3.0390\n",
      "Epoch 504/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0521\n",
      "Epoch 505/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0329\n",
      "Epoch 506/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0501\n",
      "Epoch 507/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0305\n",
      "Epoch 508/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0400\n",
      "Epoch 509/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0396\n",
      "Epoch 510/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0371\n",
      "Epoch 511/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0474\n",
      "Epoch 512/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0466\n",
      "Epoch 513/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0521\n",
      "Epoch 514/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.0489\n",
      "Epoch 515/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0606\n",
      "Epoch 516/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0722\n",
      "Epoch 517/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0621\n",
      "Epoch 518/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0615\n",
      "Epoch 519/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0523\n",
      "Epoch 520/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0553\n",
      "Epoch 521/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0702\n",
      "Epoch 522/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0531\n",
      "Epoch 523/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0481\n",
      "Epoch 524/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0562\n",
      "Epoch 525/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0622\n",
      "Epoch 526/3000\n",
      "1920/1920 [==============================] - 51s 26ms/step - loss: 3.0512\n",
      "Epoch 527/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0601\n",
      "Epoch 528/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0477\n",
      "Epoch 529/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.0613\n",
      "Epoch 530/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0462\n",
      "Epoch 531/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0554\n",
      "Epoch 532/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0590\n",
      "Epoch 533/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0684\n",
      "Epoch 534/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0615\n",
      "Epoch 535/3000\n",
      "1920/1920 [==============================] - 53s 27ms/step - loss: 3.0575\n",
      "Epoch 536/3000\n",
      "1920/1920 [==============================] - 53s 28ms/step - loss: 3.0559\n",
      "Epoch 537/3000\n",
      "1920/1920 [==============================] - 52s 27ms/step - loss: 3.0678\n",
      "Epoch 538/3000\n",
      "1920/1920 [==============================] - 49s 26ms/step - loss: 3.0713\n",
      "Epoch 539/3000\n",
      "1920/1920 [==============================] - 47s 24ms/step - loss: 3.0762\n",
      "Epoch 540/3000\n",
      "1221/1920 [==================>...........] - ETA: 16s - loss: 3.0686"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6fd1a7e30b9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-bffc367d5d22>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(load_notes, to_load_model, learning_rate)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     model.fit(\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetwork_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetwork_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machine-learning/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machine-learning/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machine-learning/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machine-learning/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machine-learning/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machine-learning/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \"\"\"\n\u001b[0;32m-> 1661\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/anaconda3/envs/machine-learning/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/envs/machine-learning/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machine-learning/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences_prediction(notes, pitchnames, n_vocab):\n",
    "\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "    sequence_length = 16\n",
    "    network_input = []\n",
    "    output = []\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        output.append(note_to_int[sequence_out])\n",
    "\n",
    "    n_patterns = len(network_input)\n",
    "\n",
    "    # reshape the input into a format compatible with LSTM layers\n",
    "    normalized_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    # normalize input\n",
    "    normalized_input = normalized_input / float(n_vocab)\n",
    "\n",
    "    return (network_input, normalized_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_notes(model, network_input, pitchnames, n_vocab):\n",
    "    \"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
    "    # Starts the melody by picking a random sequence from the input as a starting point\n",
    "    start = np.random.randint(0, len(network_input)-1)\n",
    "\n",
    "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    pattern = network_input[start]\n",
    "    prediction_output = []\n",
    "\n",
    "\n",
    "    for note_index in range(1000):\n",
    "        prediction_input = np.reshape(pattern, (1, len(pattern), 1))\n",
    "        prediction_input = prediction_input / float(n_vocab)\n",
    "\n",
    "        ### Copy the line below from your above implementation.\n",
    "        prediction = model.predict(prediction_input)\n",
    "        \n",
    "        sum = 0\n",
    "        i = 0\n",
    "        \n",
    "        for a in prediction[0]:\n",
    "            sum = sum + a\n",
    "        x = random.random() * sum\n",
    "        \n",
    "        for a in prediction[0]:\n",
    "            x = x - a\n",
    "            if(x < 0):\n",
    "                break\n",
    "            i = i+1\n",
    "\n",
    "        index = i\n",
    "        result = int_to_note[index]\n",
    "        prediction_output.append(result)\n",
    "\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "        \n",
    "    return prediction_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From: https://stackoverflow.com/questions/1806278/convert-fraction-to-float\n",
    "def convert_to_float(frac_str):\n",
    "    try:\n",
    "        return float(frac_str)\n",
    "    except ValueError:\n",
    "        num, denom = frac_str.split('/')\n",
    "        try:\n",
    "            leading, num = num.split(' ')\n",
    "            whole = float(leading)\n",
    "        except ValueError:\n",
    "            whole = 0\n",
    "        frac = float(num) / float(denom)\n",
    "        return whole - frac if whole < 0 else whole + frac\n",
    "\n",
    "def create_midi(prediction_output):\n",
    "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
    "        from the notes \"\"\"\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        pattern = pattern.split()\n",
    "        temp = pattern[0]\n",
    "        duration = pattern[1]\n",
    "        pattern = temp\n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        # pattern is a rest\n",
    "        elif('rest' in pattern):\n",
    "            new_rest = note.Rest(pattern)\n",
    "            new_rest.offset = offset\n",
    "            new_rest.storedInstrument = instrument.Piano() #???\n",
    "            output_notes.append(new_rest)\n",
    "        # pattern is a note\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += convert_to_float(duration)\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "\n",
    "    midi_stream.write('midi', fp='test_output.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate():\n",
    "    notes = pickle.load(open('notes.p', 'rb'))\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "    n_vocab = len(set(notes))\n",
    "\n",
    "    network_input, normalized_input = prepare_sequences_prediction(notes, pitchnames, n_vocab)\n",
    "    model = create_network(normalized_input, n_vocab)\n",
    "    \n",
    "    ### Add a line to load the weights here\n",
    "    \n",
    "    model.load_weights(\"weights.hdf5\")\n",
    "    \n",
    "    prediction_output = generate_notes(model, network_input, pitchnames, n_vocab)\n",
    "    create_midi(prediction_output)\n",
    "    \n",
    "def create_midi(prediction_output):\n",
    "    i = 0\n",
    "    for a in prediction_output:\n",
    "        prediction_output[i] = [int(x) for x in a.split('.')]\n",
    "        i=i+1\n",
    "    \n",
    "    t0notes = []\n",
    "    t1notes = []\n",
    "    t2notes = []\n",
    "    t9notes = []\n",
    "    \n",
    "    for a in prediction_output:\n",
    "        if(a[2] == 0):\n",
    "            pass\n",
    "        else:       \n",
    "            if( a[1] == 0 ):\n",
    "                t0notes.append([a[0],a[2]])\n",
    "            elif( a[1] == 1 ):\n",
    "                t1notes.append([a[0],a[2]])\n",
    "            elif( a[1] == 2 ):\n",
    "                t2notes.append([a[0],a[2]])\n",
    "            elif( a[1] == 9 ):\n",
    "                t9notes.append([a[0],a[2]])\n",
    "                                    \n",
    "#     print(t0notes)\n",
    "#     print(t1notes)\n",
    "#     print(t2notes)\n",
    "#     print(t9notes)\n",
    "    \n",
    "    \n",
    "    mid = MidiFile()\n",
    "    mTrack = MidiTrack()\n",
    "    track0 = MidiTrack()\n",
    "    track1 = MidiTrack()\n",
    "    track2 = MidiTrack()\n",
    "    track9 = MidiTrack()\n",
    "    mid.tracks.append(mTrack)\n",
    "    mid.tracks.append(track0)\n",
    "    mid.tracks.append(track1)\n",
    "    mid.tracks.append(track2)\n",
    "    mid.tracks.append(track9)\n",
    "\n",
    "    track0.append(Message('program_change', channel=0, program=80, time=0))\n",
    "    track1.append(Message('program_change', channel=1, program=81, time=0))\n",
    "    track2.append(Message('program_change', channel=2, program=38, time=0))\n",
    "    track9.append(Message('program_change', channel=9, program=121, time=0))\n",
    "\n",
    "    for a in t0notes:\n",
    "        track0.append(Message('note_on', channel=0, note=a[0], time=a[1]*100))\n",
    "        track0.append(Message('note_on', channel=0, note=a[0], velocity=0))\n",
    "    for a in t1notes:\n",
    "        track1.append(Message('note_on', channel=1, note=a[0], time=a[1]*100))\n",
    "        track1.append(Message('note_on', channel=1, note=a[0], velocity=0))\n",
    "    for a in t2notes:\n",
    "        track2.append(Message('note_on', channel=2, note=a[0], time=a[1]*100))\n",
    "        track2.append(Message('note_on', channel=2, note=a[0], velocity=0))\n",
    "    for a in t9notes:\n",
    "        track9.append(Message('note_on', channel=9, note=a[0], time=a[1]*100))\n",
    "        track9.append(Message('note_on', channel=9, note=a[0], velocity=0))\n",
    "    mid.save('output.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
